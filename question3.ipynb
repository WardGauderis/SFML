{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "uitleg knn -> werking: buren\n",
    "uitleg decision-tree-regr -> alg uitleg\n",
    "\n",
    "Data set:\n",
    "    split in train + test\n",
    "    uitleg cross-val (uit cursus) + extend naar time series\n",
    "\n",
    "WARD:\n",
    "KNN -> cross met train (gridsearch gedoe) -> geeft knn met beste params\n",
    "repeat for decision tree regr\n",
    "\n",
    "vgl test err op test set + plot\n",
    "\n",
    "conclusie: x beter dan y in deze situatie ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "voorspellings lijn over de tijd van test set\n",
    "\n",
    "\n",
    "how do x models compare to time series data:\n",
    "concl: knn wrs shit"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 3\n",
    "\n",
    "**How does the decision tree regressor model compare to the k-nearest neighbour regressor model in terms of in- and out-of-sample error for time series forecasting?**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training data description\n",
    "\n",
    "To investigate the research question, we use a Kaggle dataset containing many in-house measurements of temperatures and humanities, as well as weather data and appliances energy consumption. For our goal, we focus\n",
    "on predicting the humidity in the kitchen (in percentages) based on the other measurements.\n",
    "\n",
    "The data contains 15000 measurements from the period: 01/11/2016 until 24/04/2016. These are measurements of four months, of 105 different day. Each day contains 144 measure. This is one measure every ten minutes.<br>\n",
    "Below, a table can be seen that shows the data we are given. All the features contain numbers, except the feature \"date\" which is of the type datetime. This must be taken into consideration when feeding a model with this data. Not every model is can handle the datetime, so we will adapt it.\n",
    "\n",
    "| | Column | Data type |\n",
    "| --- | --- | --- |\n",
    "| 1  |  date |          datetime |\n",
    "| 2  |  T1 - T9 |       float64 |\n",
    "| 3  |  RH_1 - RH_9 |   float64 |\n",
    "| 20 |  T_out |         float64 |\n",
    "| 21 |  Press_mm_hg |   float64 |\n",
    "| 22 |  RH_out |        float64 |\n",
    "| 23 |  Windspeed |     float64 |\n",
    "| 24 |  Visibility |    float64 |\n",
    "| 25 |  Tdewpoint |     float64 |\n",
    "| 26 |  rv1 |           float64 |\n",
    "| 27 |  rv2 |           float64 |\n",
    "| 28 |  lights |        int64 |\n",
    "| 29 |  Appliances |    int64 |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialisation\n",
    "\n",
    "We set up some utility functions to create plots of the humidity over time and per hour and include the used datascience libraries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn.base\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_months(ax: plt.Axes, column: pd.DataFrame, label: str) -> None:\n",
    "\t\"\"\"\n",
    "\tPlot a data column for two months\n",
    "\t:param ax: Pyplot axis\n",
    "\t:param column: data column to plot\n",
    "\t:param label: label for the plot\n",
    "\t:return: None\n",
    "\t\"\"\"\n",
    "\tax.plot(column, label=label)\n",
    "\tax.set_xlabel(\"Time\")\n",
    "\tax.set_ylabel(\"Kitchen humidity (%)\")\n",
    "\n",
    "\n",
    "def plot_hours(ax: plt.Axes, data: pd.DataFrame, column: str) -> None:\n",
    "\t\"\"\"\n",
    "\tPlot boxplots of the data column for every hour\n",
    "\t:param ax: Pyplot axis\n",
    "\t:param data: dataframe\n",
    "\t:param column: column inside dataframe to plot\n",
    "\t:return: None\n",
    "\t\"\"\"\n",
    "\tdata.boxplot(column=[column], by='hour', ax=ax)\n",
    "\tax.set_xlabel(\"Hours\")\n",
    "\tax.set_ylabel(\"Kitchen humidity (%)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset preprocessing and exploring\n",
    "\n",
    "We read in the relevant dataset columns. _RH_1_ is the column containing kitchen humidity values which we want to predict."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Columns to read in\n",
    "columns = [\n",
    "\t\"date\",\n",
    "\t\"lights\",\n",
    "\t\"T1\", \"RH_1\",\n",
    "\t\"T2\", \"RH_2\",\n",
    "\t\"T3\", \"RH_3\",\n",
    "\t\"T4\", \"RH_4\",\n",
    "\t\"T5\", \"RH_5\",\n",
    "\t\"T6\", \"RH_6\",\n",
    "\t\"T7\", \"RH_7\",\n",
    "\t\"T8\", \"RH_8\",\n",
    "\t\"T9\", \"RH_9\",\n",
    "\t\"T_out\",\n",
    "\t\"Press_mm_hg\",\n",
    "\t\"RH_out\",\n",
    "\t\"Windspeed\",\n",
    "\t\"Visibility\",\n",
    "\t\"Tdewpoint\",\n",
    "\t\"Appliances\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To make this command work, 1) make sure you pip installed kaggle and 2) have a kaggle.json file in the correct directory.\n",
    "# More information on the kaggle.json file can be found here: https://www.kaggle.com/docs/api\n",
    "!kaggle datasets download -d loveall/appliances-energy-prediction\n",
    "\n",
    "from zipfile import ZipFile\n",
    "zf = ZipFile('appliances-energy-prediction.zip')\n",
    "zf.extractall('data-rq-3/') #save files in selected folder\n",
    "zf.close()\n",
    "\n",
    "\n",
    "# In case doing the kaggle.json setup is too much work, you can just download the csv manually from: https://www.kaggle.com/datasets/loveall/appliances-energy-prediction\n",
    "# Just make sure the KAG_energydata_complete.csv file is then saved in the directory: data-rq-3/\n",
    "data = pd.read_csv(\"data-rq-3/KAG_energydata_complete.csv\", header=0, usecols=columns)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The table contains over 5 months worth of measurements taken every 10 minutes. To speed up the training and prediction process for the experiments we limit ourselves to the first 2 full months. We also resample the time series to contain hourly measurements to smooth out very harsh fluctuations in humidity.<br>\n",
    "Since the models that we want to compare don't support datetime values, we split this column into three separate numerical month, day and hour columns. Using our existing knowledge about humidity, it is a reasonable assumption that these features \n",
    "might have a significant impact on the variable to be predicted."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the first two months of measurements\n",
    "data = data[(data['date'] > '2016-02-01 00:00:00') & (data['date'] < '2016-04-01 00:00:00')]\n",
    "\n",
    "# Split the date column into two month, day and hour columns\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data[\"month\"] = pd.to_numeric(data[\"date\"].dt.strftime(\"%m\"))\n",
    "data[\"day\"] = pd.to_numeric(data[\"date\"].dt.strftime(\"%d\"))\n",
    "data[\"hour\"] = pd.to_numeric(data[\"date\"].dt.strftime(\"%H\"))\n",
    "\n",
    "# Set the date column to be the index and resample the time series to hourly data\n",
    "data = data.set_index('date')\n",
    "data = data.resample(\"60T\").mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split the time series into a training and testing set by taking the first 75% of the data as training data and the last 25% as test data. No shuffling is done as we want to predict future values only."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split training and test data\n",
    "train_data, test_data = train_test_split(data, shuffle=False)\n",
    "# Remove/add the humidity value to be predicted\n",
    "X_train = train_data.drop([\"RH_1\"], 1)\n",
    "y_train = train_data[\"RH_1\"]\n",
    "X_test = test_data.drop([\"RH_1\"], 1)\n",
    "y_test = test_data[\"RH_1\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We plot the kitchen humidity values over time available in our training set. We can see they range between 32.5 and 52.5% and are quite erratic at first sight at this scale. A general downward trend can be observed with a peak in the middle. The values after 15/03 seem to have made an unexpected jump."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the training kitchen humidity over time\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_months(ax, train_data[\"RH_1\"], \"Actual humidity\")\n",
    "ax.set_title(\"Kitchen humidity over time (training dataset)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To investigate the influence of the passing of the day in hours on the humidity values, we create an hourly boxplot over the training data. We observe that the variation in humidity in general stays similar but rises in the afternoon and evening. Exceptionally higher humidity values are observed at 19h."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the training kitchen humidity variation every hour\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_hours(ax, train_data, \"RH_1\")\n",
    "fig.suptitle(\"Kitchen humidity per hour (training dataset)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training, Testing and Validating\n",
    "\n",
    "We set up the experiment by creating multiple functions to help us train and evaluate models and to parameter selection using cross-validation.<br>\n",
    "The first function aids in measuring the training and test score and error. As this is a regression problem we use the root-mean-square error\n",
    "$$E = \\sqrt{\\dfrac{\\sum_{n=0}^N (y_n - h(x_n))^2}{N}}$$\n",
    "The score is then calculated as follows\n",
    "$$1-\\dfrac{\\sum_{n=0}^N (y_n - h(x_n))^2}{\\sum_{n=0}^N (y_n - \\bar{y})^2}$$\n",
    "for mean $\\bar{y}$. The best possible score is thus 1 but the score may also be negative when the model performs worse than predicting the average $\\bar{y}$ over the training set. This function then also makes some comparative plots.<br>\n",
    "The second function creates and evaluates a baseline model with normalisation where the default hyperparameters are used.<br>\n",
    "The third function does a randomized grid search over a parameter grid by sampling a certain number of configurations at random without replacement. The grid search makes use of cross-validation. However, for time series prediction we can't use the standard k-fold cross-validation, so we use the method explained above. In order to not bias our comparison of the two models, the parameter selection explores an equal amount of sample configurations for both.\n",
    "The best parameter set is then used to train a model on the full training set. Finally, this model is evaluated."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model: sklearn.pipeline.Pipeline,\n",
    "                   X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.DataFrame, y_test: pd.DataFrame) -> None:\n",
    "\t\"\"\"\n",
    "\tEvaluate the model by calculating the training and test score and RMSE, plotting the predicted and actual humidity values for the test dataset and full dataset\n",
    "\tand the hourly difference between the predicted and actual humidity values\n",
    "\t:param model: model to evaluate\n",
    "\t:param X_train: training features\n",
    "\t:param X_test: test features\n",
    "\t:param y_train: training labels\n",
    "\t:param y_test: test labels\n",
    "\t:return: None\n",
    "\t\"\"\"\n",
    "\t# Calculate the training and test score and RMSE\n",
    "\tprint(f\"Training score: {model.score(X_train, y_train):.3f}, Test score: {model.score(X_test, y_test):.3f}\")\n",
    "\tprint(\n",
    "\t\tf\"Training RMSE: {mean_squared_error(y_train, model.predict(X_train), squared=False):.3f} Test RMSE: {mean_squared_error(y_test, model.predict(X_test), squared=False):.3f}\")\n",
    "\n",
    "\t# Give predictions\n",
    "\ttest_data[\"predictions\"] = model.predict(X_test)\n",
    "\ttest_data[\"error\"] = test_data[\"RH_1\"] - test_data[\"predictions\"]\n",
    "\ttrain_data[\"predictions\"] = model.predict(X_train)\n",
    "\n",
    "\t# Plot the predicted and actual humidity values for the test dataset\n",
    "\tfig, ax = plt.subplots(figsize=(20, 10))\n",
    "\tplot_months(ax, test_data[\"RH_1\"], \"Actual humidity\")\n",
    "\tplot_months(ax, test_data[\"predictions\"], \"Predicted humidity\")\n",
    "\tax.set_title(\"Kitchen humidity over time (test dataset)\")\n",
    "\tplt.legend()\n",
    "\tplt.plot()\n",
    "\n",
    "\t# Plot the predicted and actual humidity values for the full dataset\n",
    "\tfig, ax = plt.subplots(figsize=(20, 10))\n",
    "\tplot_months(ax, test_data[\"RH_1\"], \"Actual humidity (test dataset)\")\n",
    "\tplot_months(ax, test_data[\"predictions\"], \"Predicted humidity (test dataset)\")\n",
    "\tplot_months(ax, train_data[\"RH_1\"], \"Actual humidity (training dataset)\")\n",
    "\tplot_months(ax, train_data[\"predictions\"], \"Predicted humidity (training dataset)\")\n",
    "\tax.set_title(\"Kitchen humidity over time (full dataset)\")\n",
    "\tplt.legend()\n",
    "\tplt.plot()\n",
    "\n",
    "\t# Plot the hourly difference between the predicted and actual humidity values\n",
    "\tfig, ax = plt.subplots(figsize=(20, 10))\n",
    "\tplot_hours(ax, test_data, \"error\")\n",
    "\tax.set_title(\"Kitchen humidity deviation per hour (test dataset)\")\n",
    "\tfig.suptitle(\"\")\n",
    "\tplt.plot()\n",
    "\n",
    "\n",
    "def create_baseline_model(model: DecisionTreeRegressor | KNeighborsRegressor, X_train: pd.DataFrame,\n",
    "                          y_train: pd.DataFrame, X_test: pd.DataFrame,\n",
    "                          y_test: pd.DataFrame) -> sklearn.pipeline.Pipeline:\n",
    "\t\"\"\"\n",
    "\tCreate a baseline model with normalisation and evaluate it\n",
    "\t:param model: model to use\n",
    "\t:param X_train: training features\n",
    "\t:param y_train: training labels\n",
    "\t:param X_test: test features\n",
    "\t:param y_test: test labels\n",
    "\t:return: the trained pipeline\n",
    "\t\"\"\"\n",
    "\tmodel = make_pipeline(StandardScaler(), model)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\n",
    "\tevaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def randomize_grid_search(model: sklearn.pipeline.Pipeline, parameters: dict, X_train: pd.DataFrame,\n",
    "                          y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame,\n",
    "                          iterations: int) -> RandomizedSearchCV:\n",
    "\t\"\"\"\n",
    "\tRandomly search for the best hyperparameters for the given model in a given parameter grid using time series cross validation, fit the best model on the full training set and evaluate it\n",
    "\t:param model: pipeline to use\n",
    "\t:param parameters: parameter grid to search in\n",
    "\t:param X_train: training features\n",
    "\t:param y_train: training labels\n",
    "\t:param X_test: test features\n",
    "\t:param y_test: test labels\n",
    "\t:param iterations: how many samples to do\n",
    "\t:return: the tuned and trained model\n",
    "\t\"\"\"\n",
    "\tstart = time()\n",
    "\ttuner = RandomizedSearchCV(model, parameters, n_iter=iterations, n_jobs=-2, cv=TimeSeriesSplit(), verbose=1,\n",
    "\t                           scoring=\"neg_root_mean_squared_error\", random_state=0)\n",
    "\n",
    "\ttuner.fit(X_train, y_train)\n",
    "\tprint(f\"{((time() - start) / 60):.2f} minutes passed\")\n",
    "\tprint(f\"Best cross-validation RMSE: {-tuner.best_score_:.3f}\")\n",
    "\tprint(f\"Best parameters: {tuner.best_params_}\")\n",
    "\n",
    "\tevaluate_model(tuner.best_estimator_, X_train, X_test, y_train, y_test)\n",
    "\n",
    "\treturn tuner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Decision Tree Regressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We run the experiment first on a baseline decision tree regressor without configuration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a baseline decision tree regressor and evaluate it\n",
    "model = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "model = create_baseline_model(model, X_train, y_train, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The top plot visualises the true and predicted humidity values in function of the time on the test set over approximately two weeks. We that a general upward trend is correctly predicted by the model based on the features even though this trend wasn't really present in the training data. A test score of 0.479 is already obtained with a relatively low error in humidity of 1.551%. This can be recognised in the graph. Although the model is able to predict some smaller fluctuations, the actual bigger peaks cannot. However, sometimes similar peaks are predicted where they do not occur.<br>\n",
    "A score of 1 and RMSE of 0 indicate that the tree is fitting the training data perfectly. This can also be observed in the second graph. There is good chance the baseline model is actually overfitting the data and picking up noisy features. This might be resolved by applying model selection in the next step.<br>\n",
    "When looking at the third graph displaying the deviation between the actual and predicted humidity in the test set for every hour, we observe that in general the percentage is overestimate slightly but underestimated for the peak at 19h.\n",
    "\n",
    "We now select the best parameters for the decision tree regressor based on the training data using time series cross-validation. We sample 1000 configurations from the grid where we consider the following hyperparameters: splitting method,\n",
    "tree depth, minimum leaf samples, amount of features to consider at every split and the amount of leaf nodes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do a randomised grid search for the best parameters and evaluate the selected model\n",
    "parameters = {\n",
    "\t\"decisiontreeregressor__splitter\": [\"best\", \"random\"],  # Split using the squared error criterion or randomly\n",
    "\t\"decisiontreeregressor__max_depth\": [4, 8, 10, 12, 14, None],  # Maximum depth of the tree\n",
    "\t\"decisiontreeregressor__min_samples_leaf\": [1, 4, 8, 10, 12],  # Minimum number of samples required a leaf\n",
    "\t\"decisiontreeregressor__max_features\": [None, \"log2\", \"sqrt\"],\n",
    "\t# Number of features to consider when looking for the best split\n",
    "\t\"decisiontreeregressor__max_leaf_nodes\": [20, 30, 40, 50, 60, None],  # Maximum number of leaf nodes\n",
    "}\n",
    "\n",
    "model = randomize_grid_search(model, parameters, X_train, y_train, X_test, y_test, iterations=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By looking at the cross-validation RMSE, we can notice that a simpler, more regularised decision tree is able to generalise better on a dataset of this size. This is accomplished by reducing the tree depth, samples per leave and amount of leaves.<br>\n",
    "When this model is trained on the full training set it achieves an increased test score of 0.554 and reduced humidity error  of 1.434%. This is a decrease in RMSE of more than 0.1%. Looking at the graph of the test set only, this is perceived by more conservative predictions. The model predicts more flat surfaces (a result of returning the average humidity in a leaf) and is able to recognise potential peaks. However, it avoids predicting wrong extreme humidity values in contrast to the baseline model.<br>\n",
    "The regularised model doesn't fit the training data perfectly anymore but comes really close with a training score of  0.949. In the second graph this is recognisable by the fact that the predicted training humidities are less erratic.<br>\n",
    "When looking at the error in humidity per hour, we now see that the humidities are not overestimated as much anymore although the underestimation at 19h is still present with a big variance in deviation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The K-Nearest Neighbours Regressor\n",
    "\n",
    "The previous experiment is repeated for the k-nearest neighbours regressor. We first establish a baseline model again without configuration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a baseline K-Nearest Neighbors regressor and evaluate it\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "model = create_baseline_model(model, X_train, y_train, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As is clear from the first graph, this baseline nearest neighbours model is much more susceptible to the noise present in the training data. As a consequence the model's test score is only 0.040, barely better than predicting the mean training humidity. The RMSE is above 2% humidity. The predictions on the test set contain multiple big unwarranted peaks and are less in line with the general upward trend.<br>\n",
    "Looking at the predictions on the training set, we see that this model doesn't fit the humidity values perfectly (not surprising given that the default amount of neighbours is 5) but comes really close. The outlying humidity values are not fitted.<br>\n",
    "Similar to the decision tree regressor, the third graph shows that the median hourly prediction is too low while the humidity peaks around 19h are underestimated.\n",
    "\n",
    "In a similar fashion, we do parameter selection by randomly sampling 1000 parameter configurations from the grid and validating them with cross-validation. We consider the parameters relating to the amount of neighbours, the weighting of the neighbours, the power parameter for the Minkowski metric used, the type of tree datastructure to find the neighbours and the leave size within this tree."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do a randomised grid search for the best parameters and evaluate the selected model\n",
    "parameters = {\n",
    "\t\"kneighborsregressor__n_neighbors\": [1, 2, 5, 10, 20, 50, 60],  # Number of neighbours to consider\n",
    "\t\"kneighborsregressor__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "\t# The algorithm and datastructure to find the neigbours\n",
    "\t\"kneighborsregressor__weights\": [\"uniform\", \"distance\"],  # The weight function used in the prediction\n",
    "\t\"kneighborsregressor__leaf_size\": [1, 2, 5, 10, 15, 20],  # The leaf size of the tree\n",
    "\t\"kneighborsregressor__p\": [1, 2, 3, 4],  # The power of the Minkowski distance metric\n",
    "}\n",
    "\n",
    "model = randomize_grid_search(model, parameters, X_train, y_train, X_test, y_test, iterations=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameter set obtained through selection has a root-mean-square error of 2.089% humidity. Surprisingly, this configured model does fit the training set perfectly which can be observed in the second graph. This can be explained by the chosen parameter settings. Instead of uniformly weighting the importance of the neighbours, the influence of the points on the prediction is weighted by the inverse of their distance. This is combined with a Manhattan-distance instead of Euclidean distance and the amount of neighbours to be considered is increased.<br>\n",
    "In this way, the tuned model achieves a slightly increased score of 0.089, still not much better than predicting the mean. But by looking at the graph of the testing set, we see that less extreme jumps in humidity are now avoided although large fluctuations remain that are not always present in reality. This could be explained by the many jumps present in the dataset for which the Manhattan-distance is close to the test features.<br>\n",
    "The humidity deviation per hour is very similar to the baseline model, although the variance in deviation has reduced slightly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "We can conclude that for this dataset of in-house measurements and weather data, a decision tree regressor model selected through cross-validation generalises better to future predictions of humidity percentages than a k-nearest neighbour regressor selected through the same process. <br>\n",
    "The nature and inner workings of the two regression machine learning models express themselves in their future predictions.\n",
    "The predictions of the decision tree regressor were smoother with fewer peaks and contained multiple flat surfaces when the depth of the tree was limited. The k-nearest neighbour regressor on the other hand, was much more susceptible to large fluctuations and noise and it's predictions on the test set, although not correct, resembled more the actual values in the training set.<br>\n",
    "The nature of time series predictions in which we mainly try to extrapolate from the training data in the past, makes it so that similar measurements are less relevant than in a standard regression problem. This could potentially explain the poor testing accuracy of the nearest neighbours model.<br>\n",
    "We have observed that for both regression algorithms, the untuned baseline model's testing accuracy could be increased through parameter selection with time-series cross-validation. In the case of the decision tree this resulted in a more regularised model that overfitted less. For the k-nearest neighbour model, this was not as obviously visible as the training error actually disappeared while the testing predictions became more stable too.<br>\n",
    "In conclusion, our experiments indicated that the decision tree regressor is a better fit for this type of time series forecasting problem for this particular dataset and behaviour of predicted variable. The prediction accuracy of the model could potentially be improved by preprocessing the data further and removing redundant features. Cross-validation is not only important to model selection but can help in evaluating these rather manual decisions too."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}